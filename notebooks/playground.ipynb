{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmg7v7qUMA91",
        "outputId": "373ec092-6a7e-4534-e49e-43d411c1648d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    from IPython.core.getipython import get_ipython\n",
        "    get_ipython().run_line_magic(\"pip\", \"install transformers sentencepiece accelerate\")\n",
        "    get_ipython().run_line_magic(\"pip\", \"install git+https://github.com/UlisseMini/activation_additions_hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFjWy3yiMA92"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import activation_additions as aa\n",
        "\n",
        "from typing import List, Dict, Union, Callable, Tuple\n",
        "from functools import partial\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "from activation_additions.compat import ActivationAddition, get_x_vector, print_n_comparisons, pretty_print_completions, get_n_comparisons\n",
        "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
        "from functools import lru_cache\n",
        "from activation_additions.utils import colored_tokens\n",
        "from IPython.display import display, HTML\n",
        "from ipywidgets import interact, FloatSlider, IntSlider, Text, fixed\n",
        "from huggingface_hub import snapshot_download\n",
        "from html import escape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9g_nU2oMA92"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdPtgps4MA93"
      },
      "outputs": [],
      "source": [
        "device: str = \"mps\" if torch.has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "_ = torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJnE1lLkMA93"
      },
      "outputs": [],
      "source": [
        "# TODO: Model loading should go in the library, tokenizer wrapping is cursed\n",
        "\n",
        "MODEL = \"llama-13b\"\n",
        "if 'llama' in MODEL:\n",
        "    model_path: str = \"../models/llama-13B\" if MODEL == 'llama-13b' else snapshot_download(\"decapoda-research/llama-7b-hf\")\n",
        "    config = LlamaConfig.from_pretrained(model_path)\n",
        "    # decapoda-research llama is kinda fucked\n",
        "    config.update({\"bos_token_id\": 1, \"eos_token_id\": 2, \"pad_token_id\": 0})\n",
        "\n",
        "    with init_empty_weights():\n",
        "        model = AutoModelForCausalLM.from_config(config)\n",
        "        model.tie_weights() # in case checkpoint doesn't contain duplicate keys for tied weights\n",
        "\n",
        "    model = load_checkpoint_and_dispatch(model, model_path, device_map={'': device}, dtype=torch.float16, no_split_module_classes=[\"LlamaDecoderLayer\"])\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
        "    # Fancy unicode underscore doesn't overlap with normal underscore!\n",
        "    model.to_str_tokens = lambda t: [t.replace('▁', ' ') for t in tokenizer.tokenize(t)]\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "    model.to_str_tokens = lambda t: [t.replace('Ġ', ' ') for t in tokenizer.tokenize(t)]\n",
        "\n",
        "model.tokenizer = tokenizer\n",
        "# In steering experimentation spaces were found to work well, this makes no sense and I hate it.\n",
        "tokenizer.pad_token_id = int(model.tokenizer.encode(\" \")[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwMQhbSdMA93"
      },
      "outputs": [],
      "source": [
        "# Sampling kwargs for gpt2-xl, llama ideal may be different!\n",
        "sampling_kwargs: Dict[str, Union[float, int]] = {\n",
        "    \"temperature\": 1.0,\n",
        "    \"top_p\": 0.3,\n",
        "    \"freq_penalty\": 1.0,\n",
        "    \"num_comparisons\": 3,\n",
        "    \"tokens_to_generate\": 50,\n",
        "    \"seed\": 0,  # For reproducibility\n",
        "}\n",
        "get_x_vector_preset: Callable = partial(\n",
        "    get_x_vector,\n",
        "    pad_method=\"tokens_right\",\n",
        "    model=model,\n",
        "    custom_pad_id=tokenizer.pad_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJkgtP6lMA93"
      },
      "source": [
        "## Explore AVE vectors with a perplexity dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TvZwGnUMA94"
      },
      "outputs": [],
      "source": [
        "# TODO: move some of this to the library\n",
        "\n",
        "@lru_cache(maxsize=1000)\n",
        "def get_diff_vector(prompt_add: str, prompt_sub: str, layer: int):\n",
        "    return aa.get_diff_vector(model, tokenizer, prompt_add, prompt_sub, layer)\n",
        "\n",
        "\n",
        "@lru_cache\n",
        "def run_aa(coeff: float, layer: int, prompt_add: str, prompt_sub: str, texts: tuple[str], loss_ignore_mod_tokens: bool = False):\n",
        "    # todo: could compute act_diff for all layers at once, then a single fwd pass of cost for changing layer.\n",
        "    act_diff = coeff * get_diff_vector(prompt_add, prompt_sub, layer)\n",
        "    blocks = aa.get_blocks(model)\n",
        "    with aa.pre_hooks([(blocks[layer], aa.get_hook_fn(act_diff))]):\n",
        "        inputs = tokenizer(list(texts), return_tensors='pt', padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        output = model(**inputs)\n",
        "\n",
        "    logprobs = torch.log_softmax(output.logits.to(torch.float32), -1)\n",
        "    token_loss = -logprobs[..., :-1, :].gather(dim=-1, index=inputs['input_ids'][..., 1:, None])[..., 0]\n",
        "    if loss_ignore_mod_tokens:\n",
        "        loss = token_loss[..., act_diff.shape[1]:].mean(1) # skip the screwed-up modified tokens\n",
        "    else:\n",
        "        # TODO: Make generic over injection location (along with everything else...)\n",
        "        loss = token_loss.mean(1)\n",
        "\n",
        "    return loss, token_loss, logprobs\n",
        "\n",
        "\n",
        "def run_aa_interactive_diff(*args, topk=False, **kwargs):\n",
        "    assert len(kwargs['texts'][0]) == len(kwargs['texts'][1]), 'must have same number of positive/negative examples'\n",
        "    split_at = len(kwargs['texts'][0])\n",
        "    kwargs['texts'] = kwargs['texts'][0] + kwargs['texts'][1]\n",
        "\n",
        "    loss, token_loss, mod_logprobs = run_aa(*args, **kwargs)\n",
        "    abs_loss, abs_token_loss, abs_logprobs = run_aa(0., 0, '', '', texts=kwargs['texts']) # cached\n",
        "    diff, diff_token_loss = (loss - abs_loss), (token_loss - abs_token_loss)\n",
        "\n",
        "    print(f'loss change: {[round(l, 4) for l in diff.tolist()]}')\n",
        "    print(f'{(diff.argsort()[:split_at] < split_at).sum()} / {len(kwargs[\"texts\"])} most likely texts are good')\n",
        "\n",
        "    # If you have the convention that texts[0] is \"similar\" to texts[1] (e.g. \"I love you\" v.s. \"I hate you\") then\n",
        "    # a loss based on pairwise distances is interpretable.\n",
        "    # If you don't have that convention, this loss still works, just rearrange.\n",
        "    sloss = (diff[:split_at] - diff[split_at:]).mean()\n",
        "    print(f'separation loss: {sloss:.4f}')\n",
        "    print(f'change in loss: {diff.mean():.4f}')\n",
        "\n",
        "    # Negative loss gives logprobs\n",
        "    display(HTML(show_colors(\n",
        "        kwargs['texts'], abs_logprobs, mod_logprobs,\n",
        "        -diff_token_loss, topk=topk,\n",
        "        steering_prompts=(kwargs['prompt_add'], kwargs['prompt_sub']))\n",
        "    ))\n",
        "\n",
        "\n",
        "def show_colors(\n",
        "        texts: list[str],\n",
        "        logprobs_nom,\n",
        "        logprobs_mod,\n",
        "        token_logprobs_diff,\n",
        "        topk=False,\n",
        "        steering_prompts: Tuple[str, str] = None,\n",
        "):\n",
        "    # compute topk for unmodified and modified model\n",
        "    assert len(texts) == len(token_logprobs_diff)\n",
        "    assert steering_prompts is None or len(steering_prompts) == 2 # add and sub vectors\n",
        "\n",
        "    # tokenize steering prompts\n",
        "    if steering_prompts:\n",
        "        steering_ids = tokenizer(list(steering_prompts), padding=True)['input_ids']\n",
        "        steering_toks = [tokenizer.batch_decode(ids) for ids in steering_ids]\n",
        "        assert len(steering_toks[0]) == len(steering_toks[1]), 'Padding steering_toks failed'\n",
        "        steering_toks = [[escape(tok.replace(' ', \"' '\")) for tok in prompt] for prompt in steering_toks]\n",
        "\n",
        "    show_topk = topk # topk is shadowed later\n",
        "    if show_topk:\n",
        "        topk_nom, topk_mod = torch.topk(logprobs_nom, 5), torch.topk(logprobs_mod, 5)\n",
        "        seq_len = logprobs_nom.shape[1]\n",
        "\n",
        "    html = ''\n",
        "    for i, (text, logprobs_diff) in enumerate(sorted(zip(texts, token_logprobs_diff), key=lambda x: -x[1].mean().abs().item())):\n",
        "        # TODO: Specialize inside loop to another function\n",
        "\n",
        "        if show_topk:\n",
        "            topk_htmls = []\n",
        "            for topk in [topk_nom, topk_mod]:\n",
        "                # predictions shift forward one token\n",
        "                # TODO: optimize. the tokenization line is responsible for ~90% of the time in show_colors (~0.5s)\n",
        "                topk_tokens = [tokenizer.batch_decode(topk.indices[i, j]) for j in range(seq_len)]\n",
        "                topk_htmls.append([colored_tokens(topk_tokens[pos], topk.values[i][pos].exp().tolist(), inject_css=False, low=0, high=1) for pos in range(seq_len)])\n",
        "\n",
        "\n",
        "        str_tokens = model.to_str_tokens(text)\n",
        "        logprobs_diff = logprobs_diff[:len(str_tokens)]\n",
        "        colored_html = colored_tokens(\n",
        "            str_tokens,\n",
        "            logprobs_diff.tolist(),\n",
        "            [\n",
        "                f'ΔLogp: {l:.2f}<br>'\n",
        "                + (f'Steering: {steering_toks[0][t]} - {steering_toks[1][t]}<br>' if steering_prompts and t < len(steering_toks[0]) else '') # TODO: Make generic over injection location (don't assume start)\n",
        "                + (f'TopkNom: {topk_htmls[0][t]}<br>TopkMod: {topk_htmls[1][t]}' if show_topk else '')\n",
        "                for t, l in enumerate(logprobs_diff)\n",
        "            ],\n",
        "            inject_css=(i==len(texts)-1),\n",
        "        )\n",
        "        html += f'<p>ΔLoss: <b>{-logprobs_diff.mean():.2f}</b> - ' + colored_html + '</p>'\n",
        "\n",
        "    return html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHu2EpD4MA94"
      },
      "source": [
        "## Example: Love v.s. Hate\n",
        "\n",
        "Using the above tools to investigate the Love/Hate vector. Feel free to copy these cells to investigate multiple vectors at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK7J2XNLMA94",
        "outputId": "106d7c4c-4028-43cf-8b2d-0685dadc7db2",
        "colab": {
          "referenced_widgets": [
            "3392e6606db4479cacf9870b12bda59a"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3392e6606db4479cacf9870b12bda59a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Checkbox(value=False, description='Show topk (bit slow)'), FloatSlider(value=1.0, descri…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<function __main__.run_aa_interactive_diff(*args, topk=False, **kwargs)>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ipywidgets import Checkbox\n",
        "\n",
        "# using a tuple here allows hashing for cache lookup\n",
        "texts = (\n",
        "    # We want to increase the probability of these\n",
        "    ('I hate you because I love you\\nI love you', \"I hate you because you're so beautiful.\", 'I hate you because I love you\\nThe world is a stage'),\n",
        "\n",
        "    # ...And decrease these\n",
        "    ('I hate you because you are a girl.', \"I hate you because you're not me.\\nI hate you because I am me.\", 'I hate you because you are a man and I am a woman.')\n",
        ")\n",
        "\n",
        "widgets = dict(\n",
        "    coeff=FloatSlider(value=1, min=0, max=10),\n",
        "    layer=IntSlider(value=0, min=0, max=39),\n",
        "    prompt_add=Text('Love'), prompt_sub=Text('Hate'),\n",
        "    texts=fixed(texts),\n",
        "    topk=Checkbox(value=False, description='Show topk (bit slow)'),\n",
        "    loss_ignore_mod_tokens=Checkbox(value=False, description='Include modified tokens in loss')\n",
        ")\n",
        "interact(run_aa_interactive_diff, **widgets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcUNjSNSMA95"
      },
      "outputs": [],
      "source": [
        "# Once you find a good vector, attempt generation\n",
        "PROMPT = \"I hate you because\"\n",
        "\n",
        "summand: List[ActivationAddition] = [\n",
        "    *get_x_vector_preset(\n",
        "        prompt1=widgets['prompt_add'].value,\n",
        "        prompt2=widgets['prompt_sub'].value,\n",
        "        coeff=widgets['coeff'].value,\n",
        "        act_name=widgets['layer'].value,\n",
        "    )\n",
        "]\n",
        "\n",
        "kwargs = sampling_kwargs.copy()\n",
        "prompt_batch = [PROMPT] * kwargs.pop('num_comparisons')\n",
        "results = get_n_comparisons(\n",
        "    model=model,\n",
        "    prompts=prompt_batch,\n",
        "    additions=summand,\n",
        "    **kwargs,\n",
        ")\n",
        "pretty_print_completions(results=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJyhwdBDMA95",
        "outputId": "2626dcdf-2615-481f-cb26-aef9ce2d9135"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('I hate you because you are a girl.\\nYou are a girl and I am not.\\nI am not a girl and you are.\\nYou have to be the first one to say it, then we will see who is right.',\n",
              " \"I hate you because you're so beautiful and i'm so ugly\\ni hate you because i love you and i love you because i hate you\\ni wish that we could be together but then again what would people think?\\nwe can't be together\",\n",
              " \"I hate you because you're so beautiful.\\nI love you, and, in a way, I hate you.\\nI love your smile and your eyes.\\nBut I hate the fact that we can't be together.\\nI love how sweet\")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can copy results to use for loss comparison\n",
        "df = results[results.is_modified == True]\n",
        "tuple(p+c for p,c in zip(df.prompts, df.completions))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ave",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}